{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NLP Project : Resume Parsing and JD Matching** \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0lZXWhQ5CLFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Extracting text from Resume Files\n",
        "\n",
        "---\n",
        "Will extract the resume text from three different types of files : .pdf, .docx and .doc\n",
        "\n",
        "1a) To extract the text from pdf format files will use the library called pdfminer. Install pdfminer using pip\n",
        "\n",
        "1b) In order to extract the text from .docx format , will use the docx2txt library. Install docx2txt using pip\n",
        "\n",
        "1c) For extracting the text from the .doc files, will use the catdoc command line tool , which reads MS-Word and print ASCII text. Will use apt command to install the tool \n",
        "\n",
        "Now for supporting the natural langugage processing tasks like tokenization etc., will use the nltk (Natural Language toolkit) toolkit library  and for basic processing the numpy library \n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "IgV2S1QmECre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install docx2txt\n",
        "!pip install pdfminer.six\n",
        "!apt-get update yes \n",
        "!apt-get install -y catdoc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w63KiujyEA8K",
        "outputId": "39f3ad28-2d86-4a46-e5ad-83f09f4eecd4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20221105)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (40.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "E: The update command takes no arguments\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "catdoc is already the newest version (1:0.95-4.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2 : Import the required libraries and Packages\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-vGacPEiB0Ep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiicK_tqBzlU",
        "outputId": "c957dcb0-0d77-4e24-ef58-e001c0fdd72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import nltk\n",
        "import docx2txt\n",
        "import docx2txt\n",
        "import subprocess\n",
        "from pdfminer.high_level import extract_text\n",
        " \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 : Data Extraction from Resume file\n",
        "\n",
        "---\n",
        "\n",
        "3a) define a function to extract text from PDF format file\n",
        "\n",
        "3b) define a function to extract text from .docx format file\n",
        "\n",
        "3c) define a function to extract text from .doc format file"
      ],
      "metadata": {
        "id": "r__S1i6aHo0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functio to extract text from pdf file\n",
        "def pdf_txt_extraction(pdf_file_path):\n",
        "    return extract_text(pdf_file_path)"
      ],
      "metadata": {
        "id": "TcyHNRukHoRO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def docx_txt_extraction(docx_file_path):\n",
        "    txt = docx2txt.process(docx_file_path)\n",
        "    if txt:\n",
        "        return txt.replace('\\t', ' ') # replace the tabs in text with space\n",
        "    return None"
      ],
      "metadata": {
        "id": "9W0kmBfHIm8U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_txt_extraction(doc_file_path):\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            ['catdoc', '-w', doc_file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "    except (\n",
        "        FileNotFoundError,\n",
        "        ValueError,\n",
        "        subprocess.TimeoutExpired,\n",
        "        subprocess.SubprocessError,\n",
        "    ) as err:\n",
        "        return (None, str(err))\n",
        "    else:\n",
        "        stdout, stderr = process.communicate()\n",
        " \n",
        "    return (stdout.strip(), stderr.strip())"
      ],
      "metadata": {
        "id": "I9T4IbTPI9UV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Extraction of Entities from the resume data extracted\n",
        "\n",
        "---\n",
        "\n",
        "Define functins to extract entities like persone names, phone number , email , skills and educational institue names from the extracted resume data "
      ],
      "metadata": {
        "id": "unELjqq5JTE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def name_entity_extraction(txt):\n",
        "    names_person = []\n",
        " \n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                names_person.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return names_person"
      ],
      "metadata": {
        "id": "EPCukeTBJSCS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "def tuple_conversion(tup):\n",
        "    st = ''.join(map(str, tup))\n",
        "    return st\n",
        "\n",
        "def phone_no_extraction(text_resume):\n",
        "    text_resume = tuple_conversion(text_resume)\n",
        "    phone_no = re.findall(PHONE_REG, text_resume)\n",
        " \n",
        "    if phone_no:\n",
        "        number = ''.join(phone_no[0])\n",
        " \n",
        "        if text_resume.find(number) >= 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None\n",
        " "
      ],
      "metadata": {
        "id": "fq5kxR8cJuVd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        " \n",
        "def emails_extract(res_text):\n",
        "    return re.findall(EMAIL_REG, res_text)"
      ],
      "metadata": {
        "id": "q4scIhyGKlUq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "SKILLS_DB = pd.read_csv(\"/content/skills_db.csv\")\n",
        "SKILLS_DB = list(SKILLS_DB[\"Text\"])\n",
        "\n",
        "def skills_extraction(text_input):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(text_input)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    # we create a set to keep the results in.\n",
        "    skills_extracted = set()\n",
        " \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            skills_extracted.add(token)\n",
        " \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            skills_extracted.add(ngram)\n",
        " \n",
        "    return skills_extracted\n"
      ],
      "metadata": {
        "id": "NNOL4_WLK29_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESERVED_WORDS = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'univers',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "    'faculdades',\n",
        "    'Schola',\n",
        "    'schule',\n",
        "    'lise',\n",
        "    'lyceum',\n",
        "    'lycee',\n",
        "    'polytechnic',\n",
        "    'kolej',\n",
        "    'ünivers',\n",
        "    'okul',\n",
        "]\n",
        "\n",
        "\n",
        "def org_entity_extration(text_input):\n",
        "    list_organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(text_input):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                list_organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        " \n",
        "    organizations = set()\n",
        "    for org in list_organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if org.lower().find(word) >= 0:\n",
        "                organizations.add(org)\n",
        " \n",
        "    return organizations"
      ],
      "metadata": {
        "id": "UdgkWVtNLW8J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 : Process the Resume file and extract the entities from it \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnX1gPreNy-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume Parsing\n",
        "def resume_parse(file_path):\n",
        "        \n",
        "    # determine the file type \n",
        "    # unpacking the tuple\n",
        "    file_name, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "    print(\"-\"*90)\n",
        "    print(\"File Meta Data\")\n",
        "    print(file_name)\n",
        "    print(file_extension)\n",
        "    print(\"-\"*90)\n",
        "\n",
        "    if file_extension == \".docx\":\n",
        "      resume_txt = docx_txt_extraction(file_path)\n",
        "    elif file_extension == \".pdf\":\n",
        "      resume_txt = pdf_txt_extraction(file_path)\n",
        "    elif file_extension == \".doc\":\n",
        "      resume_txt,err = doc_txt_extraction(file_path)\n",
        "    else: \n",
        "      print(file_extension + \"format not supported\" )\n",
        "    \n",
        "    \n",
        "    print(\"-\"*90)\n",
        "    print(\"Raw Resume Text\")\n",
        "    #print(resume_txt)\n",
        "    print(\"-\"*90)\n",
        "\n",
        "    # extract entities\n",
        "    # Extract Name\n",
        "    names = name_entity_extraction(resume_txt) \n",
        "    if names:\n",
        "        print(\"-\"*90)\n",
        "        print(\"Name of Candidate - \")\n",
        "        print(names[0:2])\n",
        "        print(\"-\"*90)\n",
        "    else:\n",
        "        print(\"-\"*90)\n",
        "        print(\"Candidate Name not found\")\n",
        "        \n",
        "    # Extract Phone number    \n",
        "    phone_number = phone_no_extraction(resume_txt)\n",
        "    if phone_number:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Cadidate Contact number - \")\n",
        "      print(phone_number)\n",
        "      print(\"-\"*90)\n",
        "    else:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidate's contact numnber not found\")\n",
        "      print(\"-\"*90)\n",
        "      \n",
        "    # Extract Email    \n",
        "    emails = emails_extract(resume_txt) \n",
        "    if emails:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidates Email Address\")\n",
        "      print(emails)\n",
        "      print(\"-\"*90)\n",
        "    else:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidate's Email id not found\")\n",
        "      print(\"-\"*90)\n",
        "       \n",
        "    # Extract Skills\n",
        "\n",
        "    skills = skills_extraction(resume_txt)\n",
        "    if skills:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidates Skills\")\n",
        "      print(skills)\n",
        "      print(\"-\"*90)\n",
        "    else:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidate's Skills not found\")\n",
        "      print(\"-\"*90) \n",
        "\n",
        "    # Extract Organizations\n",
        "    org_information = org_entity_extration(resume_txt)\n",
        "    if org_information:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidates Skills\")\n",
        "      print(org_information)\n",
        "      print(\"-\"*90)\n",
        "    else:\n",
        "      print(\"-\"*90)\n",
        "      print(\"Candidate's Organizations not found\")\n",
        "      print(\"-\"*90)\n",
        "    \n",
        "    return skills"
      ],
      "metadata": {
        "id": "IKxjQjZpNrdk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JD Parsing\n",
        "def jd_parse(jd_file_path):\n",
        "    # determine the file type \n",
        "    # unpacking the tuple\n",
        "    jd_file_name, jd_file_extension = os.path.splitext(jd_file_path)\n",
        "\n",
        "    print(\"-\"*90)\n",
        "    print(\"JD File Meta Data\")\n",
        "    print(jd_file_name)\n",
        "    print(jd_file_extension)\n",
        "    print(\"-\"*90)\n",
        "\n",
        "    if jd_file_extension == \".docx\":\n",
        "      jd_txt = docx_txt_extraction(jd_file_path)\n",
        "    elif jd_file_extension == \".pdf\":\n",
        "      jd_txt = pdf_txt_extraction(jd_file_path)\n",
        "    elif jd_file_extension == \".doc\":\n",
        "      jd_txt,err = doc_txt_extraction(jd_file_path)\n",
        "    else: \n",
        "      print(jd_file_extension + \"format not supported\" )\n",
        "    \n",
        "    \n",
        "    print(\"-\"*90)\n",
        "    print(\"Raw JD Text\")\n",
        "    #print(jd_txt)\n",
        "    print(\"-\"*90)\n",
        "    jd_skills = skills_extraction(jd_txt)\n",
        "    if jd_skills:\n",
        "      print(\"-\"*90)\n",
        "      print(\"JD Skills\")\n",
        "      print(jd_skills)\n",
        "      print(\"-\"*90)\n",
        "    else:\n",
        "      print(\"-\"*90)\n",
        "      print(\"JD Skills not found\")\n",
        "      print(\"-\"*90) \n",
        "    return jd_skills \n",
        "\n"
      ],
      "metadata": {
        "id": "keUKT_I9_Rjd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_resume_and_jd(resume_skills, jd_skills):\n",
        "    resume_skills = set(resume_skills)\n",
        "    jd_skills = set(jd_skills)\n",
        "    \n",
        "    # Calculate the number of overlapping skills\n",
        "    matching_skills = resume_skills.intersection(jd_skills)\n",
        "    num_matching_skills = len(matching_skills)\n",
        "    \n",
        "    # Calculate the matching score\n",
        "    matching_score = num_matching_skills / len(jd_skills)\n",
        "    \n",
        "    return matching_score"
      ],
      "metadata": {
        "id": "hiEY7dm1A3zq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # parse the resume\n",
        "  resume_file_path = \"/content/Brendan_Herger_Resume.pdf\"\n",
        "  #resume_file_path = \"/content/NFI John Straumann.doc\"\n",
        "  #resume_file_path = \"/content/Bhabesh Kumar Dey Resume.docx\"\n",
        "  resume_skills = resume_parse(resume_file_path)\n",
        "\n",
        "  # Parse the JD\n",
        "  #jd_file_path = \"/content/Brendan_Herger_Resume.pdf\"\n",
        "  #jd_file_path = \"/content/NFI John Straumann.doc\"\n",
        "  jd_file_path = \"/content/JD1.docx\"\n",
        "\n",
        "  jd_skills = jd_parse(jd_file_path)\n",
        "  #match_resume_and_jd(resume_skills,jd_skills)\n",
        "  # Match resume and JD based on skills\n",
        "  matching_score = match_resume_and_jd(resume_skills, jd_skills)\n",
        "\n",
        "  print(\"Matching Score: {:.2%}\".format(matching_score))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJo1GRQUDCoV",
        "outputId": "8d33d0dc-3762-4bc0-cb26-665126ba01ae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------\n",
            "File Meta Data\n",
            "/content/Brendan_Herger_Resume\n",
            ".pdf\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Raw Resume Text\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Name of Candidate - \n",
            "['Brendan', 'Herger']\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Candidate's contact numnber not found\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Candidates Email Address\n",
            "['13herger@gmail.com']\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Candidates Skills\n",
            "{'Capital', 'Econometrics', 'real time', 'adoption', 'Positions', 'required', 'President', 'reviews', 'Sciences Mathematics', 'University', 'Experience', 'NoSQL', 'using', 'Analytics', 'Math', 'correctly', 'Implemented', 'time', 'Linear', 'search', 'package', 'Pro', 'Data Analysis', 'Structures Algorithms', 'Pandas', 'Mathematics', 'Innovation', 'Supervisor', 'Computer', 'Home Depot', 'Scientist', 'Graduate', 'Relational Databases', 'parsing', 'Data Structures', 'space', 'Regression', 'members', 'Forest', 'interface', 'End', 'Page', 'contact details', 'Time', 'held', 'Education', 'Special', 'label', 'Identified', 'Time Series', 'Supervised', 'Street', 'finds', 'Deployed', 'contact', 'May', 'The', 'of', 'Series', 'machine learning', 'class', 'Spark', 'Learning', 'lyrics', 'Structures', 'movie', 'Front', 'Boost', 'numpy', 'Lead', 'user', 'Lab', 'Development', 'Ca', 's', 'Data', 'City', 'Physics', 'Services', 'checks', 'Relational', 'built', 'SAS', 'staff', 'team', 'real', 'Python', 'classification', 'NoSQL Databases', 'Databases', 'for', 'MySQL', 'BS', 'text', 'Various Technical', 'Software Development', 'Projects', 'algorithm', 'National', 'research', 'model', 'One', 'Time Series Analysis', 'Online', 'resume', 'Society', 'Machine Learning', 'Rental', 'Regression Analysis', 'Apache', 'a', 'Created', 'fraud', 'Exploratory Data Analysis', 'Development Data', 'Relevant', 'San', 'R', 'problem', 'trained', 'keywords', 'Cashier', 'Redwood', 'Analysis', 'Adv', 'Various', 'algorithms', 'MS', 'Data Structures Algorithms', 'Hot', 'Acquisition', 'Java', 'distributed', 'Sciences', 'Apex', 'Stochastic', 'team members', 'text classification', 'Experience Data', 'Apache Spark', 'machine', 'Built', 'Technical', 'database', 'web', 'engine', 'general', 'Neural', 'Network', 'to', 'Front End', 'Support', 'user interface', 'Selected', 'Home', 'modeling', 'frameworks', 'Algebra', 'Machine', 'Random Forest', 'Projects Identified', 'Recommended', 'since', 'Vector', 'Linear Algebra', 'Naive Bayes', 'Algorithms', 'Data Scientist', 'and', 'Software', 'Personal', 'Architecture', 'details', 'Tool', 'ensemble', 'learning', 'Linear Regression', 'Data Acquisition', 'search engine'}\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Candidates Skills\n",
            "{'University Scholar'}\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "JD File Meta Data\n",
            "/content/JD1\n",
            ".docx\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "Raw JD Text\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "JD Skills\n",
            "{'find', 'strong experience', 'innovative', 'techniques', 'Statistics', 'looking', 'Experience using', 'data architectures', 'consulting digital', 'performance', 'Analytics', 'Google', 'Gurobi', 'Mathematics', 'quality', 'industry knowledge', 'working', 'improve', 'computing', 'quantitative field', 'statistical', 'product development', 'Coordinate', 'businesses', 'apply', 'decision', 'capabilities', 'mining', 'providers', 'implement', 'etc Knowledge', 'models', 'hidden', 'network', 'computer', 'tests', 'implementing', 'identify opportunities', 'Site', 'Strong', 'Qualifications', 'neural', 'proven', 'functional', 'A', 'life', 'Accenture', 'someone', 'advanced', 'proper', 'R', 'verbal communication skills', 'organization', 'testing', 'problem', 'Reduce', 'stakeholders', 'processes', 'organizations', 'testing framework', 'development', 'problem solving skills', 'machine', 'technologies', 'Computer Science', 'social', 'applications', 'right candidate', 'Facebook', 'scale', 'using advanced', 'world', 'Coding', 'knowledge', 'Develop', 'clients', 'industry', 'Consulting', 'insights', 'artificial neural networks', 'decision trees', 'gathering', 'be', 'Excellent written', 'must', 'methods', 'Excellent', 'gained', 'clustering', 'models using', 'deliver', 'outcomes', 'throughout', 'marketing', 'opportunities', 'experience', 'You', 'Job description', 'Computer', 'Redshift', 'Graduate', 'different', 'Regression', 'large data sets', 'tools', 'Hive', 'field', 'Fortune', 'of', 'll', 'Spark', 'advantages', 'Experience creating', 'neural networks', 'deep industry', 'sales leadership', 's', 'building', 'usage', 'digital technology', 'running', 'accuracy', 'languages', 'analyze', 'Python', 'development Experience using', 'for', 'text', 'ability', 'speed', 'You ll', 'digital', 'business strategies', 'framework', 'bring', 'B testing', 'several', 'technology', 'text mining', 'analyzing', 'optimization', 'opportunity', 'algorithms', 'vision', 'business', 'distributed', 'predictive modeling', 'data', 'generation', 'the', 'another', 'increase', 'teams', 'to', 'drive', 'candidate', 'Random Forest', 'issues', 'ad', 'model quality', 'Job Title', 'solving skills', 'improvement', 'Data Scientist', 'and', 'simulation', 'Join', 'communication skills', 'customer', 'B', 'Key', 'comfortable working', 'consulting digital technology', 'creating', 'Trees', 'using', 'artificial', 'use', 'guide', 'C Java', 'is', 'tree', 'following', 'monitor', 'Key responsibilities', 'coordinating', 'process optimization', 'ideal candidate', 'Title', 'large', 'Work', 'execute', 're', 'The', 'Master', 'machine learning', 'business outcomes', 'databases', 'results', 'custom', 'strong', 'you', 'solutions', 'wide range', 'Experience working', 'trees', 'Job', 'Strong problem solving', 'consulting', 'deep', 'you ll', 'product', 'statistical techniques', 'sales', 'Google Analytics', 'wide', 'SAS', 'years', 'Develop company', 'on', 'targeting', 'range', 'skills', 'network analysis', 'variety', 'courses', 'model', 'data analysis', 'web services', 'communication', 'business results', 'who', 'JD', 'leveraging', 'optimize', 'simulations', 'strategies', 'transform', 'Responsibilities', 'innovative solutions', 'Strong problem', 'machine learning techniques', 'learn', 'For', 'Java', 'and marketing', 'data models', 'PHD', 'scenario', 'web', 'Hadoop', 'properties', 'sources', 'Assess', 'Hexagon', 'drive business', 'party', 'Catalyst', 'proven ability', 'large data', 'ideal', 'etc', 'Overview', 'new', 'passion', 'Use', 'analysis methods', 'Experience', 'business solutions', 'statistical models', 'networks', 'Boosting', 'data mining', 'regression', 'identify', 'analysis', 'effectiveness', 'Knowledge', 'Scientist', 'Forest', 'demands', 'leadership', 'draw', 'social network', 'new technologies', 'emphasis', 'scenario analysis', 'action', 'concepts', 'test', 'solving', 'support', 'Map', 'services', 'revenue generation', 'responsibilities', 'Data', 'JavaScript', 'right', 'in', 'C', 'clients businesses', 'learning techniques', 'MySQL', 'data sets', 'proper usage', 'master', 'verbal communication', 'Science', 'sets', 'integrate', 'a', 'revenue', 'predictive', 'etc Experience', 'GLM', 'Mine', 'ad targeting', 'architectures', 'statistics', 'following software', 'company', 'social network analysis', 'product sales', 'experiences', 'process', 'across', 'software', 'JavaScript etc', 'modeling', 'verbal', 'functional teams', 'Insights', 'experience using', 'problem solving', 'written', 'data sources', 'familiar', 'design', 'development Experience', 'description', 'analysis etc', 'machine learning algorithms', 'computer languages', 'expertise', 'comfortable', 'learning', 'quantitative'}\n",
            "------------------------------------------------------------------------------------------\n",
            "Matching Score: 10.06%\n"
          ]
        }
      ]
    }
  ]
}